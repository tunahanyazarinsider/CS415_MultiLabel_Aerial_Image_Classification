{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach 1: Transfer Learning Baseline for Multi-Label Aerial Image Classification\n",
    "\n",
    "This notebook implements a baseline multi-label classification model using transfer learning with pre-trained CNNs.\n",
    "\n",
    "## Strategy:\n",
    "- Use pre-trained ResNet50/EfficientNet as feature extractor\n",
    "- Replace final layer with multi-label classification head\n",
    "- Train with Binary Cross-Entropy loss\n",
    "- Evaluate with multi-label metrics (micro/macro F1, mAP)\n",
    "\n",
    "## Expected Performance: 70-80% F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.13/site-packages (0.24.1)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.13/site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.13/site-packages (4.4.1)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.13/site-packages (11.1.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.13/site-packages (3.10.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.13/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/anaconda3/lib/python3.13/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.10)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision transformers datasets pillow matplotlib scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AID_MultiLabel dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load dataset from HuggingFace\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading AID_MultiLabel dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjonathan-roberts1/AID_MultiLabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset Info:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnum_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load dataset from HuggingFace\n",
    "print(\"Loading AID_MultiLabel dataset...\")\n",
    "dataset = load_dataset(\"jonathan-roberts1/AID_MultiLabel\")\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"Number of samples: {dataset['train'].num_rows}\")\n",
    "print(f\"Features: {dataset['train'].features}\")\n",
    "\n",
    "# Extract class names\n",
    "class_names = dataset['train'].features['label'].feature.names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"\\nNumber of classes: {num_classes}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze label distribution\n",
    "all_labels = dataset['train']['label']\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = np.zeros(num_classes)\n",
    "labels_per_image = []\n",
    "\n",
    "for label_list in all_labels:\n",
    "    labels_per_image.append(len(label_list))\n",
    "    for cls_idx in label_list:\n",
    "        class_counts[cls_idx] += 1\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Class frequency\n",
    "axes[0].bar(range(num_classes), class_counts)\n",
    "axes[0].set_xlabel('Class Index')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Class Distribution in Dataset')\n",
    "axes[0].set_xticks(range(num_classes))\n",
    "axes[0].set_xticklabels(range(num_classes), rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Labels per image\n",
    "axes[1].hist(labels_per_image, bins=range(1, max(labels_per_image)+2), edgecolor='black')\n",
    "axes[1].set_xlabel('Number of Labels per Image')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Labels per Image')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Average labels per image: {np.mean(labels_per_image):.2f}\")\n",
    "print(f\"Min labels per image: {np.min(labels_per_image)}\")\n",
    "print(f\"Max labels per image: {np.max(labels_per_image)}\")\n",
    "print(f\"\\nMost frequent classes:\")\n",
    "for i in np.argsort(class_counts)[-5:][::-1]:\n",
    "    print(f\"  {class_names[i]}: {int(class_counts[i])} ({class_counts[i]/len(all_labels)*100:.1f}%)\")\n",
    "print(f\"\\nLeast frequent classes:\")\n",
    "for i in np.argsort(class_counts)[:5]:\n",
    "    print(f\"  {class_names[i]}: {int(class_counts[i])} ({class_counts[i]/len(all_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "# Training: aggressive augmentation for aerial images (rotation-invariant)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=90),  # Aerial images: any rotation is valid\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n",
    "])\n",
    "\n",
    "# Validation/Test: no augmentation, only resize and normalize\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Transformations defined.\")\n",
    "print(f\"Training transform: Aggressive augmentation with rotation, flips, color jitter\")\n",
    "print(f\"Validation transform: Resize and normalize only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class AIDMultiLabelDataset(Dataset):\n",
    "    def __init__(self, images, labels, num_classes, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: List of PIL images\n",
    "            labels: List of label indices (multi-label)\n",
    "            num_classes: Total number of classes\n",
    "            transform: Torchvision transforms to apply\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.num_classes = num_classes\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get image\n",
    "        image = self.images[idx]\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "        \n",
    "        # Convert to RGB if needed\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Convert labels to multi-hot encoding\n",
    "        label_vector = torch.zeros(self.num_classes, dtype=torch.float32)\n",
    "        label_vector[self.labels[idx]] = 1.0\n",
    "        \n",
    "        return image, label_vector\n",
    "\n",
    "print(\"Custom Dataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset: 70% train, 15% validation, 15% test\n",
    "images = dataset['train']['image']\n",
    "labels = dataset['train']['label']\n",
    "\n",
    "# First split: 70% train, 30% temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    images, labels, test_size=0.3, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: split temp into 50-50 for val and test (15% each of total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset split:\")\n",
    "print(f\"  Training samples: {len(X_train)} ({len(X_train)/len(images)*100:.1f}%)\")\n",
    "print(f\"  Validation samples: {len(X_val)} ({len(X_val)/len(images)*100:.1f}%)\")\n",
    "print(f\"  Test samples: {len(X_test)} ({len(X_test)/len(images)*100:.1f}%)\")\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = AIDMultiLabelDataset(X_train, y_train, num_classes, transform=train_transform)\n",
    "val_dataset = AIDMultiLabelDataset(X_val, y_val, num_classes, transform=val_transform)\n",
    "test_dataset = AIDMultiLabelDataset(X_test, y_test, num_classes, transform=val_transform)\n",
    "\n",
    "print(\"\\nDataset objects created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 32  # Adjust based on GPU memory\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-label classification model using pre-trained CNN backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, backbone='resnet50', pretrained=True, dropout=0.5):\n",
    "        super(MultiLabelCNN, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.backbone_name = backbone\n",
    "        \n",
    "        # Load pre-trained backbone\n",
    "        if backbone == 'resnet50':\n",
    "            self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()  # Remove original FC layer\n",
    "        \n",
    "        elif backbone == 'resnet34':\n",
    "            self.backbone = models.resnet34(pretrained=pretrained)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        elif backbone == 'resnet101':\n",
    "            self.backbone = models.resnet101(pretrained=pretrained)\n",
    "            num_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "        \n",
    "        # Multi-label classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\"Model initialized:\")\n",
    "        print(f\"  Backbone: {backbone} (pretrained={pretrained})\")\n",
    "        print(f\"  Feature dimension: {num_features}\")\n",
    "        print(f\"  Output classes: {num_classes}\")\n",
    "        print(f\"  Dropout: {dropout}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "    \n",
    "    def get_num_trainable_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Model class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "MODEL_BACKBONE = 'resnet50'  # Options: 'resnet34', 'resnet50', 'resnet101'\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "model = MultiLabelCNN(\n",
    "    num_classes=num_classes, \n",
    "    backbone=MODEL_BACKBONE, \n",
    "    pretrained=True,\n",
    "    dropout=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nModel moved to {device}\")\n",
    "print(f\"Total parameters: {model.get_num_params():,}\")\n",
    "print(f\"Trainable parameters: {model.get_num_trainable_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy Loss (suitable for multi-label)\n",
    "# BCEWithLogitsLoss combines sigmoid + BCE for numerical stability\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"Loss function: BCEWithLogitsLoss\")\n",
    "print(\"  - Combines Sigmoid + BCE for numerical stability\")\n",
    "print(\"  - Suitable for multi-label classification\")\n",
    "print(\"  - Treats each label independently\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Learning Rate Scheduler\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler: reduce LR when validation loss plateaus\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"\\nScheduler: ReduceLROnPlateau\")\n",
    "print(f\"  Factor: 0.5\")\n",
    "print(f\"  Patience: 3 epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation/test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc='Evaluating'):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get probabilities and predictions\n",
    "            probs = torch.sigmoid(outputs)  # Convert logits to probabilities\n",
    "            preds = (probs > threshold).float()  # Apply threshold\n",
    "            \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    return epoch_loss, all_preds, all_labels, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive multi-label classification metrics.\n",
    "    \"\"\"\n",
    "    # Overall metrics\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    weighted_f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    micro_precision = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    macro_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    micro_recall = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    macro_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    hamming = hamming_loss(y_true, y_pred)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    per_class_f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    per_class_precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    per_class_recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    \n",
    "    # Subset accuracy (exact match)\n",
    "    subset_acc = np.mean(np.all(y_true == y_pred, axis=1))\n",
    "    \n",
    "    metrics = {\n",
    "        'micro_f1': micro_f1,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'micro_precision': micro_precision,\n",
    "        'macro_precision': macro_precision,\n",
    "        'micro_recall': micro_recall,\n",
    "        'macro_recall': macro_recall,\n",
    "        'hamming_loss': hamming,\n",
    "        'subset_accuracy': subset_acc,\n",
    "        'per_class_f1': per_class_f1,\n",
    "        'per_class_precision': per_class_precision,\n",
    "        'per_class_recall': per_class_recall\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Pretty print metrics.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  Micro F1:      {metrics['micro_f1']:.4f}\")\n",
    "    print(f\"  Macro F1:      {metrics['macro_f1']:.4f}\")\n",
    "    print(f\"  Weighted F1:   {metrics['weighted_f1']:.4f}\")\n",
    "    print(f\"\\n  Micro Precision: {metrics['micro_precision']:.4f}\")\n",
    "    print(f\"  Macro Precision: {metrics['macro_precision']:.4f}\")\n",
    "    print(f\"\\n  Micro Recall:    {metrics['micro_recall']:.4f}\")\n",
    "    print(f\"  Macro Recall:    {metrics['macro_recall']:.4f}\")\n",
    "    print(f\"\\n  Hamming Loss:    {metrics['hamming_loss']:.4f}\")\n",
    "    print(f\"  Subset Accuracy: {metrics['subset_accuracy']:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 25\n",
    "EARLY_STOP_PATIENCE = 7\n",
    "\n",
    "# For tracking\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_micro_f1': [],\n",
    "    'val_macro_f1': [],\n",
    "}\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_model_path = 'best_model_baseline.pth'\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Early stopping patience: {EARLY_STOP_PATIENCE}\")\n",
    "print(f\"Best model will be saved to: {best_model_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_preds, val_labels, val_probs = evaluate(\n",
    "        model, val_loader, criterion, device, threshold=0.5\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_metrics = calculate_metrics(val_labels, val_preds, class_names)\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_micro_f1'].append(val_metrics['micro_f1'])\n",
    "    history['val_macro_f1'].append(val_metrics['macro_f1'])\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "    print(f\"  Val Micro F1: {val_metrics['micro_f1']:.4f}\")\n",
    "    print(f\"  Val Macro F1: {val_metrics['macro_f1']:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"  Current LR: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['micro_f1'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['micro_f1']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_f1': best_val_f1,\n",
    "            'metrics': val_metrics\n",
    "        }, best_model_path)\n",
    "        print(f\"  âœ“ New best model saved! (Micro F1: {best_val_f1:.4f})\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  No improvement for {epochs_no_improve} epoch(s)\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best validation Micro F1: {best_val_f1:.4f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# F1 scores\n",
    "axes[1].plot(history['val_micro_f1'], label='Micro F1', marker='o')\n",
    "axes[1].plot(history['val_macro_f1'], label='Macro F1', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('Validation F1 Scores')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history visualized and saved to 'training_history.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model for testing...\")\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Best model from epoch {checkpoint['epoch']+1} loaded.\\n\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_preds, test_labels, test_probs = evaluate(\n",
    "    model, test_loader, criterion, device, threshold=0.5\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "test_metrics = calculate_metrics(test_labels, test_preds, class_names)\n",
    "\n",
    "# Print results\n",
    "print_metrics(test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance visualization\n",
    "per_class_data = {\n",
    "    'Class': class_names,\n",
    "    'F1': test_metrics['per_class_f1'],\n",
    "    'Precision': test_metrics['per_class_precision'],\n",
    "    'Recall': test_metrics['per_class_recall']\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(class_names))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, per_class_data['Precision'], width, label='Precision', alpha=0.8)\n",
    "ax.bar(x, per_class_data['Recall'], width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width, per_class_data['F1'], width, label='F1 Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Per-Class Performance on Test Set', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Per-class performance visualized and saved to 'per_class_performance.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed per-class results table\n",
    "print(\"\\nDetailed Per-Class Results:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Class':<20} {'Precision':>12} {'Recall':>12} {'F1 Score':>12} {'Support':>10}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    support = int(test_labels[:, i].sum())\n",
    "    print(f\"{class_name:<20} {test_metrics['per_class_precision'][i]:>12.4f} \"\n",
    "          f\"{test_metrics['per_class_recall'][i]:>12.4f} \"\n",
    "          f\"{test_metrics['per_class_f1'][i]:>12.4f} {support:>10}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identify best and worst performing classes\n",
    "best_idx = np.argmax(test_metrics['per_class_f1'])\n",
    "worst_idx = np.argmin(test_metrics['per_class_f1'])\n",
    "\n",
    "print(f\"\\nBest performing class: {class_names[best_idx]} (F1: {test_metrics['per_class_f1'][best_idx]:.4f})\")\n",
    "print(f\"Worst performing class: {class_names[worst_idx]} (F1: {test_metrics['per_class_f1'][worst_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, device, num_samples=8, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualize model predictions on sample images.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Randomly select samples\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, ax in zip(indices, axes):\n",
    "        image, label = dataset[idx]\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            image_input = image.unsqueeze(0).to(device)\n",
    "            output = model(image_input)\n",
    "            probs = torch.sigmoid(output).cpu().numpy()[0]\n",
    "            pred = (probs > threshold).astype(int)\n",
    "        \n",
    "        # Convert image for display\n",
    "        img_display = image.cpu().numpy().transpose(1, 2, 0)\n",
    "        # Denormalize\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img_display = std * img_display + mean\n",
    "        img_display = np.clip(img_display, 0, 1)\n",
    "        \n",
    "        # Get labels\n",
    "        true_labels = [class_names[i] for i in range(num_classes) if label[i] == 1]\n",
    "        pred_labels = [class_names[i] for i in range(num_classes) if pred[i] == 1]\n",
    "        \n",
    "        # Display\n",
    "        ax.imshow(img_display)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        title = f\"True: {', '.join(true_labels)}\\nPred: {', '.join(pred_labels)}\"\n",
    "        ax.set_title(title, fontsize=8, wrap=True)\n",
    "        \n",
    "        # Add colored border (green=correct, red=incorrect)\n",
    "        if set(true_labels) == set(pred_labels):\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor('green')\n",
    "                spine.set_linewidth(3)\n",
    "        else:\n",
    "            for spine in ax.spines.values():\n",
    "                spine.set_edgecolor('red')\n",
    "                spine.set_linewidth(3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_samples.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(model, test_dataset, device, num_samples=8, threshold=0.5)\n",
    "print(\"Prediction samples visualized and saved to 'prediction_samples.png'\")\n",
    "print(\"Green border = Correct prediction, Red border = Incorrect prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Threshold Optimization (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal threshold on validation set\n",
    "print(\"Searching for optimal threshold...\\n\")\n",
    "\n",
    "# Get validation predictions (already computed)\n",
    "_, val_preds_default, val_labels, val_probs = evaluate(\n",
    "    model, val_loader, criterion, device, threshold=0.5\n",
    ")\n",
    "\n",
    "# Try different thresholds\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "f1_scores = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    preds = (val_probs > thresh).astype(int)\n",
    "    f1 = f1_score(val_labels, preds, average='micro')\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find best threshold\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_f1 = np.max(f1_scores)\n",
    "\n",
    "print(f\"Optimal threshold: {best_threshold:.2f}\")\n",
    "print(f\"Micro F1 at optimal threshold: {best_f1:.4f}\")\n",
    "print(f\"Micro F1 at default threshold (0.5): {f1_scores[8]:.4f}\")\n",
    "print(f\"Improvement: {best_f1 - f1_scores[8]:.4f}\")\n",
    "\n",
    "# Plot threshold analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, f1_scores, marker='o', linewidth=2)\n",
    "plt.axvline(best_threshold, color='r', linestyle='--', label=f'Optimal: {best_threshold:.2f}')\n",
    "plt.axvline(0.5, color='g', linestyle='--', label='Default: 0.50')\n",
    "plt.xlabel('Threshold', fontsize=12)\n",
    "plt.ylabel('Micro F1 Score', fontsize=12)\n",
    "plt.title('Threshold Optimization on Validation Set', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('threshold_optimization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate on test set with optimal threshold\n",
    "print(f\"\\nRe-evaluating on test set with optimal threshold ({best_threshold:.2f})...\\n\")\n",
    "\n",
    "test_preds_optimized = (test_probs > best_threshold).astype(int)\n",
    "test_metrics_optimized = calculate_metrics(test_labels, test_preds_optimized, class_names)\n",
    "\n",
    "print(\"\\nComparison: Default (0.5) vs Optimized Threshold\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<25} {'Default (0.5)':>20} {'Optimized':>20}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Micro F1':<25} {test_metrics['micro_f1']:>20.4f} {test_metrics_optimized['micro_f1']:>20.4f}\")\n",
    "print(f\"{'Macro F1':<25} {test_metrics['macro_f1']:>20.4f} {test_metrics_optimized['macro_f1']:>20.4f}\")\n",
    "print(f\"{'Micro Precision':<25} {test_metrics['micro_precision']:>20.4f} {test_metrics_optimized['micro_precision']:>20.4f}\")\n",
    "print(f\"{'Micro Recall':<25} {test_metrics['micro_recall']:>20.4f} {test_metrics_optimized['micro_recall']:>20.4f}\")\n",
    "print(f\"{'Hamming Loss':<25} {test_metrics['hamming_loss']:>20.4f} {test_metrics_optimized['hamming_loss']:>20.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BASELINE MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel Architecture: {MODEL_BACKBONE}\")\n",
    "print(f\"Total Parameters: {model.get_num_params():,}\")\n",
    "print(f\"Training Samples: {len(train_dataset)}\")\n",
    "print(f\"Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"Test Samples: {len(test_dataset)}\")\n",
    "print(f\"\\nBest Validation Micro F1: {best_val_f1:.4f}\")\n",
    "print(f\"Test Micro F1 (threshold=0.5): {test_metrics['micro_f1']:.4f}\")\n",
    "print(f\"Test Micro F1 (optimized threshold={best_threshold:.2f}): {test_metrics_optimized['micro_f1']:.4f}\")\n",
    "print(f\"Test Macro F1: {test_metrics_optimized['macro_f1']:.4f}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NEXT STEPS FOR IMPROVEMENT:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Try different backbones (EfficientNet, ResNet101, Vision Transformer)\")\n",
    "print(\"2. Implement class-weighted loss or Focal Loss to handle class imbalance\")\n",
    "print(\"3. Advanced data augmentation (Mixup, CutMix, AutoAugment)\")\n",
    "print(\"4. Add attention mechanisms (CBAM, SENet)\")\n",
    "print(\"5. Implement label correlation modeling with GNN\")\n",
    "print(\"6. Build ensemble of multiple models\")\n",
    "print(\"7. Add Grad-CAM for interpretability\")\n",
    "print(\"8. Per-class threshold optimization\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to a dictionary\n",
    "results = {\n",
    "    'model_config': {\n",
    "        'backbone': MODEL_BACKBONE,\n",
    "        'num_classes': num_classes,\n",
    "        'dropout': DROPOUT_RATE,\n",
    "        'total_params': model.get_num_params(),\n",
    "    },\n",
    "    'training_config': {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'num_epochs_trained': len(history['train_loss']),\n",
    "        'early_stop_patience': EARLY_STOP_PATIENCE,\n",
    "    },\n",
    "    'dataset_split': {\n",
    "        'train_size': len(train_dataset),\n",
    "        'val_size': len(val_dataset),\n",
    "        'test_size': len(test_dataset),\n",
    "    },\n",
    "    'test_metrics_default_threshold': test_metrics,\n",
    "    'test_metrics_optimized_threshold': test_metrics_optimized,\n",
    "    'optimal_threshold': float(best_threshold),\n",
    "    'training_history': history,\n",
    "    'class_names': class_names,\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "\n",
    "with open('baseline_results.json', 'w') as f:\n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    results_serializable = results.copy()\n",
    "    for key in ['test_metrics_default_threshold', 'test_metrics_optimized_threshold']:\n",
    "        for metric_key, metric_value in results_serializable[key].items():\n",
    "            if isinstance(metric_value, np.ndarray):\n",
    "                results_serializable[key][metric_key] = metric_value.tolist()\n",
    "    \n",
    "    json.dump(results_serializable, f, indent=2)\n",
    "\n",
    "print(\"Results saved to 'baseline_results.json'\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  - best_model_baseline.pth (model checkpoint)\")\n",
    "print(\"  - baseline_results.json (all metrics and config)\")\n",
    "print(\"  - training_history.png (loss and F1 curves)\")\n",
    "print(\"  - per_class_performance.png (per-class metrics)\")\n",
    "print(\"  - prediction_samples.png (sample predictions)\")\n",
    "print(\"  - threshold_optimization.png (threshold analysis)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
